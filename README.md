# PROYECTO WEB SCRAPING PARA FACTORIA F5

### En este tercer proyecto de Factoria F5, se me pide que realice un web scraping de una pÃ¡gina web de mi gusto.

He decidido hacer scraping la pÃ¡gina de Infojobs para ver las ofertas de trabajo disponibles segÃºn la palabra buscada.

El objetivo es tener un frontend donde podamos ingresar la palabra que queramos y que el scraper haga su trabajo y nos traiga todas las ofertas disponibles.

ðŸ”¹ Datos clave a obtener:<br>
âœ… Nombre de la oferta<br>
âœ… Empresa que lo sube<br>
âœ… Url de Infojobs de la empresa<br>
âœ… Ciudad<br>
âœ… Modalidad<br>
âœ… Salario<br>
âœ… Tipo de contrato<br>
âœ… Tipo de jornada<br>

â€¼ï¸ Problemas con los que me he encontrado:
- Al entrar a scrapear, sÃ³lo me traÃ­a las primeras 5 ofertas que aparecÃ­a en la pÃ¡gina, pero yo veÃ­a que en realidad habÃ­a 23. La soluciÃ³n fue medir el alto del contenedor de 1 oferta de trabajo, y hacer scroll lentamente usando ese heigh multiplicado por 4.
- Cuando empecÃ© a escribir el cÃ³digo, optÃ© por usar Mozila Firefox, pero Infojobs enseguida se dio cuenta que estaba usando una automatizaciÃ³n con selenium, asÃ­ que decidÃ­ usar Chrome con las dependencias user_agent y undetected_chromedriver.
- Cada vez que el scraping empezaba, me saltaba el modal de la politica de cookies, y esperaba a ser aceptado manualmente, si no lo aceptaba el script se detenÃ­a con error. Gracias a una compaeÃ±era, entendÃ­ que ese 'click' tambiÃ©n podrÃ­a automatizar. 

TODO:
- No puedo traer la pagina web de la empresa, solo la url de Infojobs. Crear una nueva tabla que conecte con el id de la empresa, entrar a su url de linkedin mediante otro view nuevo, extraer la pÃ¡gina web para luego poder mostrarlo en html.

- Cuando se haga la llamada, quitar la aperura del navegador.


> [!NOTE]
>
> Tuve un pequeÃ±o problema con Github porque dentro de mi repositorio local, creÃ© sin querer otro repositorio lo que me daba conflictos a la hora de hacer commits. Alfinal pude borrar y desvincular dicho repositorio de mi local y terminÃ© creando una nueva rama feature/scraping-v2 para seguir trabajando.
>