# Proyecto Web Scraping para Factor√≠a F5

En este tercer proyecto de **Factor√≠a F5**, se me pidi√≥ realizar un **web scraping** de una p√°gina web de mi elecci√≥n. He decidido hacer scraping en **InfoJobs** para obtener las ofertas de trabajo disponibles seg√∫n una palabra clave de b√∫squeda.

### Descripci√≥n

El objetivo del proyecto es crear un scraper que recoja las ofertas de trabajo de InfoJobs seg√∫n un t√©rmino ingresado por el usuario en un frontend. Los datos clave que se extraen son:

üîπ **Datos clave a obtener**:
- Nombre de la oferta
- Empresa que lo sube
- URL de InfoJobs de la empresa
- Ciudad
- Modalidad
- Salario
- Tipo de contrato
- Tipo de jornada


Para llevar a cabo este proyecto he usado las siguientes herramientas:
- Lenguaje Python
- Librer√≠a selenium para scrapear
- Chromedriver para scrapear en Google Chrome, y Geckodriver para scrapear en Firefox
- Supabase.com para base de datos postgres
- Librer√≠a logging de Python para la trazabilidad de logs y errores
- Test unitarios para el modelo y para los views, en scraper > tests.py
- Para evitar baneos de IP, he usado user_agent y undetected_chromedriver
- He usado Tailwind para el frontend
- Imagen Docker subido a Docker Hub para poder descargar de forma f√°cil
- Trello para la organizaci√≥n de mi proyecto
<br>

En este proyecto, he decidido hacer todo en ingl√©s (excepto el README); los nombres de variables, github commits, notas en trello, tabla base de datos, etc.
<br>
![Puedes ver las notas que he tomado en Trello durante el proyecto a medida que avanzaba o me encontraba con alg√∫n probelema](images/captura-trello.png)

> [!NOTE]
>
> En las tarjetas de Trello, puedes ver que hay etiquetas por colores, desde nivel b√°sico hasta nivel avanzado seg√∫n los requisitios de este proyecto.

> [!NOTE]
> 
> Encontrar√°s varias ramas en mi repositorio github:
> - **main**: rama principal, actualizado y funcionando
> - **feature/scraping-v1**: rama donde empec√© a hacer el c√≥digo
> - **feature/scraping-v2**: rama de la √∫ltima versi√≥n, donde he ido trabajando y mergeandolo todo en main a medida que funcionaba todo de forma correcta
> - **feature/docker**: rama donde he integrado Docker

## ¬øC√≥mo descargar y probar mi proyecto?

### Paso 1:
Descargar mi repositorio en tu local:<br>
```bash
git clone https://github.com/alharuty/Project-Web-Scraping.git
```

### Paso 2:
Entra en el repositorio:<br>
```bash
cd Project-Web-Scraping
```

### Paso 3: 
Crea un entorno virtual y act√≠valo:<br>
```bash 
python3 -m venv .venv
```
```bash
source .venv/bin/activate
````

### Paso 4:
Descarga todas las dependencias necesarias:<br>
```bash
pip install -r requirements.txt
```

### Paso 5:
Renombra el archivo .env.example por .env , e inserta los datos que te he dado.

### Paso 6:
Pon en marcha el proyecto:<br>
```bash
python scraper_project/manage.py runserver
```

### Paso 7: 
Entra en http://127.0.0.1:8000/ y realiza tu b√∫squeda de trabajo.


### URLs disponibles

- **B√∫squeda**: [http://127.0.0.1:8000/search](http://127.0.0.1:8000/search) - Realiza una b√∫squeda con el t√©rmino que desees.
- **Ofertas**: [http://127.0.0.1:8000/offers](http://127.0.0.1:8000/offers) - Muestra las ofertas de trabajo relacionadas con tu b√∫squeda o todas las b√∫squedas realizadas (disponibles en la base de datos).
- **Error**: [http://127.0.0.1:8000/error](http://127.0.0.1:8000/error) - P√°gina de error cuando algo no sale bien.


### Diagrama de actividad

![Diagrama de actividad del proyecto](./images/Activity-Diagram-Web-Scraper.png)

### Demo del proyecto

[Haz click aqu√≠ para ver la Demostraci√≥n](https://www.canva.com/design/DAGijP_bTMA/DbQRFCVzr6SFNPLYF_K7Jw/edit?utm_content=DAGijP_bTMA&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)

### Comandos para ejecutar tests unitarios

Para ejecutar los tests unitarios, usa los siguientes comandos:

```bash
python scraper_project/manage.py test scraper.tests.test_models --keepdb
```
```bash
python scraper_project/manage.py test scraper.tests.test_views --keepdb
```

Se usa ``--keepdb``para que cada vez que corremos los test no cree una nueva base de datos y as√≠ no dar errores.


### Problemas encontrados y soluciones

‚ÄºÔ∏è **Problemas encontrados**:
1. **Problema con el n√∫mero de ofertas**: Inicialmente, el scraper solo tra√≠a las primeras 5 ofertas, aunque la p√°gina mostraba 23. La soluci√≥n fue calcular la altura del contenedor de una oferta de trabajo y realizar un scroll lento usando esa altura multiplicada por 4 (la cantidad que se ve por defecto por pantalla).
   
2. **Detecci√≥n de automatizaci√≥n**: Al usar Mozilla Firefox, InfoJobs detect√≥ que estaba utilizando Selenium, por lo que decid√≠ cambiar a Chrome con las dependencias `user_agent` y `undetected_chromedriver` para evitar la detecci√≥n.

3. **Modal de cookies**: El script se deten√≠a si no aceptaba el modal de cookies manualmente. Gracias a una compa√±era, entend√≠ que tambi√©n pod√≠a automatizar ese 'click' para aceptar las cookies.

4. **Dockerizaci√≥n**: Al intentar dockerizar el proyecto con Chrome, encontr√© algunos errores. Por eso, opt√© por dockerizar la versi√≥n con Firefox utilizando el argumento `--headless`, pero debido a que InfoJobs detecta el scraping en Firefox, mi imagen no puede realizar b√∫squedas, solo muestra el historial de b√∫squedas anteriores en caso de que exista.

### Pr√≥ximas mejoras

- Actualmente no puedo obtener la p√°gina web de la empresa, solo la URL de InfoJobs. Mi plan es crear una nueva tabla que conecte con el ID de la empresa, entrar a la url de Infojobs de la empresa, extraer su p√°gina web principal mediante otro **view** nuevo, y mostrarla en el frontend en formato HTML.
- A√±adir filtros en la p√°gina offers, para que el usuario pueda filtrar por salario, por ciudad, etc.
- A√±adir gr√°fico que muestre una media de salarios.
- Structura POO.
- Automatizar script con Cronjob.
- Traducir el README a ingl√©s.
- Scrapear las paginas >1.
<br>

> [!NOTE]
>
> Tuve un peque√±o problema con Github, ya que dentro de mi repositorio local cre√© sin querer otro repositorio, lo que me generaba conflictos a la hora de hacer commits. Finalmente, pude borrar y desvincular el repositorio local y cre√© una nueva rama `feature/scraping-v2` para continuar trabajando.



> [!NOTE]
>
> Es cierto que el la imagen de mi Docker no funciona correctamente para hacer b√∫squedas, pero como s√≠ que devuelve la lista de las anteriores b√∫squedas desde la base de datos, he querido dejarlo subido para mostrar que he podido subirlo, y posteriormente podr√© corregir el error.

[Link a la im√°gen de Docker](https://hub.docker.com/r/allaharuty/scraper-v2)

## ¬øC√≥mo descargar y usar mi imagen desde Docker Hub?

Para poder ejecutar este proyecto mediante Docker, sigue estos pasos:

- Aseg√∫rate de tener instalado **Docker** en tu m√°quina. Si no lo tienes, puedes descargarlo e instalarlo desde [aqu√≠](https://www.docker.com/get-started).

- Adem√°s, abre **Docker Desktop** y aseg√∫rate de que Docker est√© en ejecuci√≥n.

### Descargar la imagen de Docker
Para descargar la imagen del proyecto desde Docker Hub, abre tu terminal y ejecuta el siguiente comando:

```bash
docker pull allaharuty/scraper-v2:latest
```

### Ejecutar el contenedor:
Una vez descargada la imagen, puedes ejecutar el contenedor con el siguiente comando:

```bash
docker run -d --name scraper-v2 allaharuty/scraper-v2:latest
```

### Verificar el funcionamiento:
Para comprobar que el contenedor se est√° ejecutando correctamente, puedes ver los logs con el siguiente comando:
```bash
docker logs scraper-v2
```

### Detener despu√©s de usar:
Cuando hayas terminado de usarlo, puedes detener el contenedor con:
```bash
docker stop scraper-v2
```

### Eliminar despu√©s de detener:
Y si deseas eliminar el contenedor despu√©s de detenerlo, ejecuta:
```bash
docker rm scraper-v2
```
