# Proyecto Web Scraping para Factor√≠a F5

En este tercer proyecto de **Factor√≠a F5**, se me pidi√≥ realizar un **web scraping** de una p√°gina web de mi elecci√≥n. He decidido hacer scraping en **InfoJobs** para obtener las ofertas de trabajo disponibles seg√∫n una palabra clave de b√∫squeda.

### Descripci√≥n

El objetivo del proyecto es crear un scraper que recoja las ofertas de trabajo de InfoJobs seg√∫n un t√©rmino ingresado por el usuario en un frontend. Los datos clave que se extraen son:

üîπ **Datos clave a obtener**:
- Nombre de la oferta
- Empresa que lo sube
- URL de InfoJobs de la empresa
- Ciudad
- Modalidad
- Salario
- Tipo de contrato
- Tipo de jornada

### Problemas encontrados y soluciones

‚ÄºÔ∏è **Problemas encontrados**:
1. **Problema con el n√∫mero de ofertas**: Inicialmente, el scraper solo tra√≠a las primeras 5 ofertas, aunque la p√°gina mostraba 23. La soluci√≥n fue calcular la altura del contenedor de una oferta de trabajo y realizar un scroll lento usando esa altura multiplicada por 4 (la cantidad que se ve por defecto por pantalla).
   
2. **Detecci√≥n de automatizaci√≥n**: Al usar Mozilla Firefox, InfoJobs detect√≥ que estaba utilizando Selenium, por lo que decid√≠ cambiar a Chrome con las dependencias `user_agent` y `undetected_chromedriver` para evitar la detecci√≥n.

3. **Modal de cookies**: El script se deten√≠a si no aceptaba el modal de cookies manualmente. Gracias a una compa√±era, entend√≠ que tambi√©n pod√≠a automatizar ese 'click' para aceptar las cookies.

4. **Dockerizaci√≥n**: Al intentar dockerizar el proyecto con Chrome, encontr√© algunos errores. Por eso, opt√© por dockerizar la versi√≥n con Firefox utilizando el argumento `--headless`, pero debido a que InfoJobs detecta el scraping en Firefox, mi imagen no puede realizar b√∫squedas, solo muestra el historial de b√∫squedas anteriores en caso de que exista.

### Pr√≥ximas mejoras

- Actualmente no puedo obtener la p√°gina web de la empresa, solo la URL de InfoJobs. Mi plan es crear una nueva tabla que conecte con el ID de la empresa, extraer la URL de su LinkedIn mediante otro **view** nuevo, y as√≠ obtener su p√°gina web para luego mostrarla en el frontend en formato HTML.
- A√±adir filtros en la p√°gina offers, para que el usuario pueda filtrar por salario, por ciudad, etc.
- A√±adir gr√°fico que muestre una media de salarios.

> **Nota**:  
> Tuve un peque√±o problema con Github, ya que dentro de mi repositorio local cre√© sin querer otro repositorio, lo que me generaba conflictos a la hora de hacer commits. Finalmente, pude borrar y desvincular el repositorio local y cre√© una nueva rama `feature/scraping-v2` para continuar trabajando.

### URLs disponibles

- **B√∫squeda**: [http://127.0.0.1:8000/search](http://127.0.0.1:8000/search) - Realiza una b√∫squeda con el t√©rmino que desees.
- **Ofertas**: [http://127.0.0.1:8000/offers](http://127.0.0.1:8000/offers) - Muestra las ofertas de trabajo relacionadas con tu b√∫squeda o todas las b√∫squedas realizadas (disponibles en la base de datos).
- **Error**: [http://127.0.0.1:8000/error](http://127.0.0.1:8000/error) - P√°gina de error cuando algo no sale bien.

### Comandos para ejecutar tests unitarios

Para ejecutar los tests unitarios, usa los siguientes comandos:

```bash
python manage.py test scraper.tests.test_models --keepdb
python manage.py test scraper.tests.test_views --keepdb
```


Se usa ``--keepdb``para que cada vez que corremos los test no cree una nueva base de datos y as√≠ no dar errores.


### Diagrama de actividad

![Diagrama de actividad del proyecto](./images/Activity-Diagram-Web-Scraper.png)

### Demo del proyecto

Demo here...

### Usar mi imagen desde Docker Hub

Para poder ejecutar este proyecto mediante Docker, sigue estos pasos:

- Aseg√∫rate de tener instalado **Docker** en tu m√°quina. Si no lo tienes, puedes descargarlo e instalarlo desde [aqu√≠](https://www.docker.com/get-started).

- Adem√°s, abre **Docker Desktop** y aseg√∫rate de que Docker est√© en ejecuci√≥n.

#### Descargar la imagen de Docker
Para descargar la imagen del proyecto desde Docker Hub, abre tu terminal y ejecuta el siguiente comando:

```bash
docker pull allaharuty/scraper:latest
```

#### Ejecutar el contenedor:
Una vez descargada la imagen, puedes ejecutar el contenedor con el siguiente comando:

```bash
docker run -d --name scraper allaharuty/scraper:latest
```

#### Verificar el funcionamiento:
Para comprobar que el contenedor se est√° ejecutando correctamente, puedes ver los logs con el siguiente comando:
```bash
docker logs scraper
```

#### Detener despu√©s de usar:
Cuando hayas terminado de usarlo, puedes detener el contenedor con:
```bash
docker stop scraper
```

#### Eliminar despu√©s de detener:
Y si deseas eliminar el contenedor despu√©s de detenerlo, ejecuta:
```bash
docker rm scraper
```
