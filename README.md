# PROYECTO WEB SCRAPING PARA FACTORIA F5

### En este tercer proyecto de Factoria F5, se me pide que realice un web scraping de una página web de mi gusto.

He decidido hacer scraping la página de Infojobs para ver las ofertas de trabajo disponibles según la palabra buscada.

El objetivo es tener un frontend donde podamos ingresar la palabra que queramos y que el scraper haga su trabajo y nos traiga todas las ofertas disponibles.

🔹 Datos clave a obtener:<br>
✅ Nombre de la oferta<br>
✅ Empresa que lo sube<br>
✅ Url de Infojobs de la empresa<br>
✅ Ciudad<br>
✅ Modalidad<br>
✅ Salario<br>
✅ Tipo de contrato<br>
✅ Tipo de jornada<br>

‼️ Problemas con los que me he encontrado:
- Al entrar a scrapear, sólo me traía las primeras 5 ofertas que aparecía en la página, pero yo veía que en realidad había 23. La solución fue medir el alto del contenedor de 1 oferta de trabajo, y hacer scroll lentamente usando ese heigh multiplicado por 4.
- Cuando empecé a escribir el código, opté por usar Mozila Firefox, pero Infojobs enseguida se dio cuenta que estaba usando una automatización con selenium, así que decidí usar Chrome con las dependencias user_agent y undetected_chromedriver.
- Cada vez que el scraping empezaba, me saltaba el modal de la politica de cookies, y esperaba a ser aceptado manualmente, si no lo aceptaba el script se detenía con error. Gracias a una compaeñera, entendí que ese 'click' también podría automatizar. 

TODO:
- No puedo traer la pagina web de la empresa, solo la url de Infojobs. Crear una nueva tabla que conecte con el id de la empresa, entrar a su url de linkedin mediante otro view nuevo, extraer la página web para luego poder mostrarlo en html.

- Cuando se haga la llamada, quitar la aperura del navegador.


> [!NOTE]
>
> Tuve un pequeño problema con Github porque dentro de mi repositorio local, creé sin querer otro repositorio lo que me daba conflictos a la hora de hacer commits. Alfinal pude borrar y desvincular dicho repositorio de mi local y terminé creando una nueva rama feature/scraping-v2 para seguir trabajando.
>


**URLS DISPONIBLES**

http://127.0.0.1:8000/search => Realizar nuestra búsqueda
http://127.0.0.1:8000/offers => Lista de ofertas de trabajo con nuestro término de búsqueda o todas las búsquedas que hemos rezliado (disponibles en base de datos)
http://127.0.0.1:8000/error => Página de error cuando algo no sale bien


Para comprobar los test unitarios/unittest usaremos los siguientes comandos:<br>
``python manage.py test scraper.tests.test_models --keepdb``
``python manage.py test scraper.tests.test_views --keepdb``

Se usa ``--keepdb``para que cada vez que corremos los test no cree una nueva base de datos y así no dar errores.



DESCARGAR LA IMAGEN DESDE DOCKER HUB:

[MI URL](https://hub.docker.com/r/allaharuty/scraper)
`docker pull allaharuty/scraper`